{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iU-wmX4TkSh4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "agDgW4eplUKl",
    "outputId": "12570037-2c98-4968-9750-be231d0d333d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in c:\\users\\сократ\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in c:\\users\\сократ\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in c:\\users\\сократ\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\users\\сократ\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pymorphy2) (0.6.2)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pymorphy2\n",
    "\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "parser = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tlIwfLcPk2ZZ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:10: SyntaxWarning: invalid escape sequence '\\]'\n",
      "<>:10: SyntaxWarning: invalid escape sequence '\\]'\n",
      "C:\\Users\\Сократ\\AppData\\Local\\Temp\\ipykernel_18052\\1493062947.py:10: SyntaxWarning: invalid escape sequence '\\]'\n",
      "  punctuation = '!\\\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~—»«...–'\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "# Загрузка файла\n",
    "url = \"https://raw.githubusercontent.com/dhhse/dh2020/master/data/stop_ru.txt\"\n",
    "response = requests.get(url)\n",
    "rus_stops = response.text.splitlines()\n",
    "\n",
    "punctuation = '!\\\"#$%&\\'()*+,-./:;<=>?@[\\]^_`{|}~—»«...–'\n",
    "filter_token = rus_stops + list(punctuation)\n",
    "\n",
    "parser = MorphAnalyzer()\n",
    "\n",
    "def preprocess_gemsim(input_text):\n",
    "    text = input_text.lower()\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    clean_text = [word for word in tokenized_text if word not in filter_token]\n",
    "    lemmatized_text = [parser.parse(word)[0].normal_form for word in clean_text if word]  # Проверка на пустые строки\n",
    "    return lemmatized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lOcPINEpkuKW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Открытие файла с явным указанием кодировки\n",
    "with open(\"anek_utf8.txt\", \"r\", encoding=\"utf-8\") as file1:\n",
    "    lines = []\n",
    "    for line in file1:\n",
    "        if line.strip():  # Проверка на пустую строку\n",
    "            lines.append(line.strip())  # Удаление пробелов и символов новой строки\n",
    "\n",
    "line_new = []\n",
    "for i in range(len(lines) - 1):  # Используем 'i' вместо 'n'\n",
    "    if lines[i][:15] == '<|startoftext|>':\n",
    "        if lines[i + 1][:15] != '<|startoftext|>':\n",
    "            line_new.append(lines[i][15:] + ' ' + lines[i + 1][15:])\n",
    "        else:\n",
    "            line_new.append(lines[i][15:])\n",
    "\n",
    "# Создание DataFrame\n",
    "ds_line = pd.DataFrame(line_new, columns=['text_base'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ibs43SVtkscd",
    "outputId": "1e72d93e-34d2-4f9a-a56c-c4b5229cded1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Сократ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Сократ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')#для табов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvan-OFJkrBW",
    "outputId": "c8821738-54c5-4d17-8979-b9f706cde85c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Сократ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# ds_line[\"text_processed_new\"] = ds_line[\"text_base\"].apply(lambda text: preprocess_sklearn(text))\n",
    "ds_line[\"text_processed\"] = ds_line[\"text_base\"].apply(lambda text: preprocess_gemsim(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "zeJ4I3AkkYrd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер словаря до фильтрации: 83955\n",
      "Размер словаря после фильтрации: 37948\n"
     ]
    }
   ],
   "source": [
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "# Build the bigram  models\n",
    "texts=(ds_line[\"text_processed\"])\n",
    "from gensim import corpora, models\n",
    "\n",
    "bigram = models.Phrases(texts, min_count=3, threshold=5)\n",
    "bigram_mod = models.phrases.Phraser(bigram)\n",
    "texts_bi = make_bigrams(texts)\n",
    "dictionary_bi = corpora.Dictionary(texts_bi)                 # составляем словарь из терминов с учетом биграмм слов предлжений\n",
    "print('Размер словаря до фильтрации: {}'.format(len(dictionary_bi)))\n",
    "dictionary_bi.filter_extremes(no_below=3, no_above=0.4, keep_n=3*10**6)\n",
    "print('Размер словаря после фильтрации: {}'.format(len(dictionary_bi)))\n",
    "corpus_bi = [dictionary_bi.doc2bow(text) for text in texts_bi]  # составляем корпус документов по словарю полученному с использованием биграмм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_topics=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel_bi=lda_model_gensim(corpus_bi,dictionary_bi,number_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ldamodel_base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m topics_gensim_base \u001b[38;5;241m=\u001b[39m \u001b[43mldamodel_base\u001b[49m\u001b[38;5;241m.\u001b[39mshow_topics(num_topics\u001b[38;5;241m=\u001b[39mnumber_topics, num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,formatted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      2\u001b[0m topics_gensim_bi \u001b[38;5;241m=\u001b[39m ldamodel_bi\u001b[38;5;241m.\u001b[39mshow_topics(num_topics\u001b[38;5;241m=\u001b[39mnumber_topics, num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,formatted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtopics_base_cloud\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ldamodel_base' is not defined"
     ]
    }
   ],
   "source": [
    "# topics_gensim_base = ldamodel_base.show_topics(num_topics=number_topics, num_words=100,formatted=False)\n",
    "topics_gensim_bi = ldamodel_bi.show_topics(num_topics=number_topics, num_words=100,formatted=False)\n",
    "with open('topics_base_cloud', 'wb') as f:\n",
    "    pickle.dump(topics_gensim_base, f)\n",
    "with open('topics_bi_cloud', 'wb') as f:\n",
    "    pickle.dump(topics_gensim_bi, f)\n",
    "\n",
    "topics_base=pd.DataFrame(topics_gensim_base)\n",
    "topics_base.columns=['topic num','topic_word']\n",
    "topics_bi=pd.DataFrame(topics_gensim_bi)\n",
    "topics_bi.columns=['topic num','topic_word']"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
